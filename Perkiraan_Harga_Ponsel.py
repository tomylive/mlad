# -*- coding: utf-8 -*-
"""Prediksi_Harga_Ponsel.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1_9B2fKOtuxmmgSmaYBgKYWcXDPr8-6aR

#**Business Understanding**

**Mobile Price Klasifikasi**

Perusahaan telah memulai perusahaan selulernya sendiri. Ia ingin memberikan perlawanan keras kepada perusahaan-perusahaan besar.

Perusahaan tidak tahu bagaimana memperkirakan harga ponsel yang dibuat oleh perusahaannya. Di pasar ponsel yang kompetitif ini, Anda tidak bisa begitu saja berasumsi. Untuk memecahkan masalah ini, Perusahaan mengumpulkan data penjualan ponsel dari berbagai perusahaan.

Perusahaan ingin mengetahui hubungan antara fitur-fitur ponsel (misalnya: RAM, Memori Internal, dll.) dan harga jualnya. Namun, ia tidak begitu ahli dalam Machine Learning. Jadi, ia butuh bantuan Anda untuk memecahkan masalah ini.

**Problem Statements dan Goals**

*   Bagaimana Memprediksi harga sebenarnya tetapi kisaran harga yang menunjukkan seberapa tinggi harganya?
*   Bagaimana mengetahui hubungan antara fitur-fitur ponsel (misalnya: RAM, Memori Internal, dll.) dan harga jualnya?

**Membuat predictive modelling dengan tujuan atau goals sebagai berikut:**

*   Mengetahui fitur-fitur yang berkorelasi dan tidak berkolerasi dengan price_range.
*   Membuat model machine learning yang dapat memprediksi kisaran harga dengan seakurat mungkin berdasarkan fitur-fitur yang ada.

#**Data Understanding**

*   Data loading
*   Exploratory Data Analysis - Deskripsi Variabel
*   Exploratory Data Analysis - Menangani Missing Value dan Outliers
*   Exploratory Data Analysis - Univariate Analysis
*   Exploratory Data Analysis - Multivariate Analysis

###**Load Data**

Sebelum melakukan load data terlebih dahulu menyiapkan library yang dibutuhkan.
"""

# Commented out IPython magic to ensure Python compatibility.
import numpy as np
import matplotlib.pyplot as plt
import pandas as pd
# %matplotlib inline
import seaborn as sns

"""Langkah selanjutnya melakukan pengambilan dataset dari situs kaggle, langkah awal upload token yang dihasilkan dari situs kaggle dengan file kaggle.json."""

# Import module yang disediakan google colab untuk kebutuhan upload file
from google.colab import files
files.upload()

"""Setelah upload file kaggle.json selanjutnya menghubungkan ke dalam API kaggle untuk pengambilan dataset."""

!mkdir -p ~/.kaggle
!cp kaggle.json ~/.kaggle/
!chmod 600 ~/.kaggle/kaggle.json

"""Code dibawah merupakan proses pengambilan dataset pada kaggle dengan link: https://www.kaggle.com/datasets/iabhishekofficial/mobile-price-classification

adapun hasil pengambilan akan didownload kedalam google drive dan langsung dilakukan unzip karena data yang terambil akan ber format .zip.
"""

!kaggle datasets download iabhishekofficial/mobile-price-classification -p "/content/drive/MyDrive/Machine Learning/Latihan Dicoding ML" --unzip

"""Setelah berhasil mendownload data selanjutnya adalah melakukan pembuatan direktory dataset untuk memudahkan pemanggilan data, berikut codenya:"""

#Membuat direktory dataset
df_train = pd.read_csv('/content/drive/MyDrive/Machine Learning/Latihan Dicoding ML/train.csv')
df_test = pd.read_csv('/content/drive/MyDrive/Machine Learning/Latihan Dicoding ML/test.csv')

#Melihat total data train dan test
print(df_train.shape)
print(df_test.shape)

"""Berdasarkan direktory data mobile price klasifikasi terdapat dua data yaitu data train dan data test. Total data berjumlah 3000 data dengan rincian sebagai berikut:
*   Data train = 2000 data dengan 21 kolom
*   Data uji = 1000 data dengan 21 kolom

Selanjutnya kita akan lihat isi masing-masing data.


"""

#Melihat isi data train
df_train.head()

#Melihat data test
df_test.head()

"""##**Exploratory Data Analysis-Deskripsi Variabel**

**Deskripsi Variabel**

Berdasarkan informasi dari Kaggle, variabel-variabel pada Mobile Price Klasifikasi adalah sebagai berikut:
1.	battery_power: Total energi yang dapat disimpan baterai dalam satu waktu diukur dalam mAh
2.	blue: Memiliki bluetooth atau tidak
3.	clock_speed: kecepatan mikroprosesor mengeksekusi instruksi
4.	dual_sim: Memiliki dukungan dual sim atau tidak
5.	fc: Mega piksel Kamera Depan
6.	four_g: Memiliki 4G atau tidak
7.	int_memory: Memori Internal dalam Gigabyte
8.	m_dep: Kedalaman Seluler dalam cm
9.	mobile_wt: Berat ponsel
10.	n_cores: Jumlah inti prosesor
11.	pc: Mega piksel Kamera Utama
12.	px_height: Tinggi Resolusi Piksel
13.	px_width: Lebar Resolusi Piksel
14.	ram: Memori Akses Acak dalam Mega Byte
15.	sc_h: Tinggi Layar ponsel dalam cm
16.	sc_w: Lebar Layar ponsel dalam cm
17.	talk_time: waktu terlama yang dapat digunakan untuk satu kali pengisian daya baterai
18.	three_g: Memiliki 3G atau tidak
19.	touch_screen: Memiliki layar sentuh atau tidak
20.	wifi: Memiliki wifi atau tidak
21.	price_range: Ini adalah variabel target dengan nilai 0 (biaya rendah), 1 (biaya sedang), 2 (biaya tinggi) dan 3 (biaya sangat tinggi).

sebagai informasi tambahan untuk kolom atau variabel antara data train dan test memiliki variabel yang sama hanya saja pada data test tidak ada variabel price_range dan diganti menjadi variabel ID.
"""

#Melihat type data train
df_train.info()

"""Berdasarkan hasil diatas dapat diketahui bahwa type data pada data train memilki 2 type data yaitu float dan int dimana variabel clock_speed dan m_dep bertipe data float dan sisanya bertipe data int.

selanjutnya akan melihat deskripsi statistik data train.
"""

#Melihat deskripsi statistik data trian.
df_train.describe()

"""##**Exploratory Data Analysis - Melihat Missing Value dan Outliers**
Selain melihat deskripsi statistik data kita juga bisa melihat missing value dari dataset train.
"""

#Melihat Missing value data train
df_train.isnull().sum()

"""Hasil diatas menunjukkan bahwa dataset pada data train tidak memiliki data missing atau data hilang, langkah selanjutnya adalah melihat outlier pada data train."""

#Melihat data outlier dengan visualisasi boxplot
num_cols = len(df_train.columns)
num_rows = (num_cols + 3) // 4  # Calculate rows needed, ensuring at least 1
df_train.plot(kind='box', subplots=True, layout=(num_rows, 4), figsize=(15, num_rows * 4))  # Adjust figsize for better visualization
plt.tight_layout()  # Add this line to prevent overlapping of subplots
plt.show()

"""Setelah kita melihat data outlier dan terlihat pesebaran data cukup baik, selanjutnya melakukan visualisasi diagram bar untuk mengetahui penyebaran atau jumlah data pada masing-masing variabel.

##**Exploratory Data Analysis - Univariate Analysis**
Dalam langkah ini saya ingin melihat persebaran data pada seluruh variabel dan melihat persebaran data pada masing-masing variabel.
"""

#Melihat jumlah data pada setiap variabel data train dengan visualisasi diagram bar
# Calculate the count of data points for each variable
variable_counts = df_train.count()

# Create a bar chart
plt.figure(figsize=(12, 6))  # Adjust figure size if needed
plt.bar(variable_counts.index, variable_counts.values)
plt.xlabel("Variables")
plt.ylabel("Count")
plt.title("Count of Data Points for Each Variable in Training Dataset")
plt.xticks(rotation=90)  # Rotate x-axis labels for better readability
plt.tight_layout()
plt.show()

"""**Melihat penyebaran atau jumlah data pada masing-masing variabel data train.**"""

import matplotlib.pyplot as plt
import pandas as pd

# Ambil semua kolom numerik
numeric_cols = df_train.select_dtypes(include=np.number).columns.tolist()

# Buat histogram terpisah untuk setiap kolom
fig, axes = plt.subplots(len(numeric_cols), 1, figsize=(10, 4*len(numeric_cols)))

for i, col in enumerate(numeric_cols):
  axes[i].hist(df_train[col], bins=20) # Anda dapat menyesuaikan jumlah bins sesuai kebutuhan
  axes[i].set_title(f'Penyebaran Data {col}')
  axes[i].set_xlabel(col)
  axes[i].set_ylabel('Frekuensi')

plt.tight_layout()
plt.show()

#melihat histogram masing-masing fitur
df_train.hist(bins=50, figsize=(20,15))
plt.show()

"""##**Exploratory Data Analysis - Multivariate Analysis**"""

# Pilih semua kolom numerik sebagai fitur
num_features = df_train.select_dtypes(include=np.number).columns.tolist()
num_features.remove('price_range') # Hapus price_range dari fitur karena merupakan target variabel

# Loop melalui setiap fitur dan buat catplot
for col in num_features:
  sns.catplot(x=col, y="price_range", kind="bar", data=df_train, height=4, aspect=3, palette="Set3")
  # Menampilkan estimator=np.mean untuk rata-rata
  plt.title(f"Rata-rata 'price_range' Relatif terhadap - {col}")
  plt.show()

# Mengamati hubungan antar fitur numerik dengan fungsi pairplot()
sns.pairplot(df_train, diag_kind = 'kde')

#Melihat korelasi
# Menghitung matriks korelasi
correlation_matrix = df_train.corr()

# Membuat heatmap
plt.figure(figsize=(15, 10))  # Adjust the figure size if needed
sns.heatmap(correlation_matrix, annot=True, cmap='coolwarm', fmt=".2f")
plt.title("Matriks Korelasi")
plt.show()

"""Berdasarkan hasil korelasi diatas dapat diketahui variabel yang memiliki korelasi paling kuat adalah battery_power dan ram hal ini sangat dapat dipahami karena kebutuhan akan battery dan sangat berpengaruh terhadap kinerja HP, namun pada kolerasi diata terdapat variabel yang tidak memilki korelasi dengan price_range yaitu variabel n_cores, m_dep, dan clock_speed, sehingga kita dapat melakukan drop atau penghapusan pada tiga variabel tersebut.

##**Data Preparation**

Berdasarkan hasil korelasi diatas dapat diketahui bahwa terdapat variabel yang tidak memiliki korelasi dengan variabel label yaitu:
*   variabel n_cores, m_dep, dan clock_speed, sehingga saya akan melakukan drop atau penghapusan pada tiga variabel tersebut.
"""

#Menghapus fitur yang memiliki korelasi kecil yaitu fitur "n_cores, m_dep, dan clock_speed"
df_train = df_train.drop(['n_cores', 'm_dep', 'clock_speed'], axis=1)
df_train.head()

"""Langkah selanjutnya melakukan pembagian data atau split data pada data train dengan perbandingan 80:20, kemudian melakukan standarisasi data."""

#pembagian data train sebesar 80:20
from sklearn.model_selection import train_test_split
X = df_train.drop(['price_range'], axis=1)
y = df_train['price_range']
from sklearn.model_selection import train_test_split

# Bagi dataset menjadi 80% data latih dan 20% data uji
X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)

# Cek jumlah data latih dan uji
print(f"Jumlah data latih: {len(X_train)}")
print(f"Jumlah data uji: {len(X_test)}")

#Melakukan standarisasi dengan StandardScaler pada data latih carat, table, dimension.
from sklearn.preprocessing import StandardScaler

scaler = StandardScaler()
X_train_scaled = scaler.fit_transform(X_train)
X_test_scaled = scaler.transform(X_test)

#Menampilkan Hasil standarisasi dengan tabel
X_train_scaled = pd.DataFrame(X_train_scaled, columns=X_train.columns)
X_test_scaled = pd.DataFrame(X_test_scaled, columns=X_test.columns)
X_train_scaled.head()

"""##**Model Development**

Dalam melakukan pemodelan Mobile Price Klasifikasi saya memilih model klasifikasi karena variabel target berupa kalsifikasi rentang harga dengan 0 (biaya rendah), 1 (biaya sedang), 2 (biaya tinggi) dan 3 (biaya sangat tinggi).

Adapun model klasifikasi yang akan saya pilih adalah **Random Forest klasifikasi dan Support Vektor Machine** dengan melakukan optimasi pada kedua model tersebut menggunakan **Hyperparameter GridSearch.**

**1. Model Random Forest**
"""

from sklearn.preprocessing import StandardScaler
from sklearn.metrics import classification_report, confusion_matrix
from sklearn.ensemble import RandomForestClassifier

# Inisialisasi model
model_rf = RandomForestClassifier(n_estimators=100, random_state=42)

# Latih model dengan data latih
model_rf.fit(X_train_scaled, y_train)

"""**Evaluasi Model Random Forest**"""

# Prediksi pada data uji
y_pred_rf = model_rf.predict(X_test_scaled)

# Evaluasi model
print(classification_report(y_test, y_pred_rf))
print(confusion_matrix(y_test, y_pred_rf))

"""Dari hasil evaluasi diatas model random forest memiliki **hasil akurasi sebesar 89%.** hasil ini akan kita tingkatkan dengan optimasi menggunakan Hyperparameter GridSearch.

**Confusion Matrix.**
"""

#Melihat Confusion Matrix dengan warna cerah
cm = confusion_matrix(y_test, y_pred_rf)
sns.heatmap(cm, annot=True, fmt='d', cmap='Blues')
plt.title('Confusion Matrix')
plt.xlabel('Predicted Labels')
plt.ylabel('True Labels')
plt.show()

"""**2. Hyperparameter Model Random Forest dengan GridSearch**

Parameter yang digunakan untuk optimasi model random forest menggunakan GridSearch yaitu:

*   'n_estimators': [50, 100, 200]
*   'max_depth': [None, 10, 20, 30]
*   'min_samples_split': [2, 5, 10]

dari parameter diatas akan dicari nilai parameter terbaik menggunakan GridSearch untuk model klasifikasi random forest.
"""

from sklearn.model_selection import GridSearchCV

param_grid = {
    'n_estimators': [50, 100, 200],
    'max_depth': [None, 10, 20, 30],
    'min_samples_split': [2, 5, 10]
}

grid_search = GridSearchCV(estimator=model_rf, param_grid=param_grid, cv=5)
grid_search.fit(X_train_scaled, y_train)

# Best parameters dan evaluasi ulang model
print(grid_search.best_params_)
y_pred_best_rf = grid_search.best_estimator_.predict(X_test_scaled)
print(classification_report(y_test, y_pred_best_rf))

"""**Hasil parameter terbaik dari Hyperparameter GridSearch yaitu:**

*   'max_depth': 20
*   'min_samples_split': 5
*   'n_estimators': 200

Dari hasil optimasi menggunakan Hyperparameter GridSearch dapat diketahui peningkatan dari **hasil akurasi sebesar 3% yaitu dari 89% menjadi 91%.** Peningkatan ini tejadi dari parameter terbaik yang dihasilkan.
"""

#Melihat Confusion Matrix dengan warna cerah
cm = confusion_matrix(y_test, y_pred_best_rf)
sns.heatmap(cm, annot=True, fmt='d', cmap='Blues')
plt.title('Confusion Matrix')
plt.xlabel('Predicted Labels')
plt.ylabel('True Labels')
plt.show()

"""**3. Model Support Vektor Machine**"""

from sklearn.svm import SVC
# Inisialisasi model SVM
model_svm = SVC(kernel='rbf', random_state=42)

# Latih model
model_svm.fit(X_train, y_train)

# Prediksi data uji
from sklearn.metrics import accuracy_score, classification_report

y_pred_svm = model_svm.predict(X_test)

# Evaluasi performa model
print(f"Akurasi: {accuracy_score(y_test, y_pred_svm)}")
print(classification_report(y_test, y_pred_svm))

"""Dari hasil evaluasi diatas model SVM memiliki **hasil akurasi sebesar 96%.** hasil yang cukup tinggi dan melebihi nilai akurasi dari model random forest. Selanjutnya hasil ini akan kita tingkatkan dengan optimasi yang sama sebelumnya yaitu menggunakan Hyperparameter GridSearch.

**Confusion Matrix**
"""

#Confusion Matrix Model SVM
cm = confusion_matrix(y_test, y_pred_svm)
sns.heatmap(cm, annot=True, fmt='d', cmap='Blues')
plt.title('Confusion Matrix')
plt.xlabel('Predicted Labels')
plt.ylabel('True Labels')

"""**4. Hyperparameter model Support Vektor Machine dengan GridSearch**

Parameter yang digunakan untuk optimasi model SVM menggunakan GridSearch yaitu:

*   'C': [0.1, 1, 10, 100]
*   'gamma': [1, 0.1, 0.01, 0.001]
*   'kernel': ['rbf', 'poly', 'sigmoid']
      
dari parameter diatas akan dicari nilai parameter terbaik menggunakan GridSearch untuk model klasifikasi SVM.
"""

from sklearn.model_selection import GridSearchCV
from sklearn.svm import SVC
from sklearn.metrics import classification_report, accuracy_score

# Tentukan parameter yang ingin di-tuning
param_grid = {
    'C': [0.1, 1, 10, 100],
    'gamma': [1, 0.1, 0.01, 0.001],
    'kernel': ['rbf', 'poly', 'sigmoid']
}

# Lakukan Grid Search dengan 5-fold cross-validation
grid = GridSearchCV(model_svm, param_grid, refit=True, verbose=2, cv=5)
grid.fit(X_train, y_train)

# Cetak parameter terbaik yang ditemukan oleh GridSearchCV
print("Best Hyperparameters:", grid.best_params_)

# Prediksi dengan model terbaik
y_pred_SVM = grid.predict(X_test)

# Evaluasi hasil prediksi
print(f"Akurasi: {accuracy_score(y_test, y_pred_SVM)}")
print(classification_report(y_test, y_pred_SVM))

"""**Hasil parameter terbaik dari Hyperparameter GridSearch yaitu:**

*   'C': 0.1
*   'gamma': 1
*   'kernel': 'poly'

Dari hasil optimasi menggunakan Hyperparameter GridSearch dapat diketahui peningkatan dari **hasil akurasi sebesar 1% yaitu dari 96% menjadi 97%.** Peningkatan ini tejadi dari parameter terbaik yang dihasilkan.

**Confusion Matrix Hyperparameter SVM**
"""

#Confusion Matrix Hyperparameter SVM
cm = confusion_matrix(y_test, y_pred_SVM)
sns.heatmap(cm, annot=True, fmt='d', cmap='Blues')
plt.title('Confusion Matrix')
plt.xlabel('Predicted Labels')
plt.ylabel('True Labels')

"""**Visualisasi Hasil Evaluasi**"""

import matplotlib.pyplot as plt
import numpy as np
from sklearn.metrics import accuracy_score

# Akurasi sebelum hyperparameter tuning
accuracy_rf_before = accuracy_score(y_test, y_pred_rf)
accuracy_svm_before = accuracy_score(y_test, y_pred_svm)

# Akurasi setelah hyperparameter tuning
accuracy_rf_after = accuracy_score(y_test, y_pred_best_rf)
accuracy_svm_after = accuracy_score(y_test, y_pred_SVM)

# Data untuk visualisasi
model_names = ['Random Forest', 'SVM']
accuracies_before = [accuracy_rf_before, accuracy_svm_before]
accuracies_after = [accuracy_rf_after, accuracy_svm_after]

# Buat bar chart
bar_width = 0.35
index = np.arange(len(model_names))

plt.figure(figsize=(10, 6))
plt.bar(index, accuracies_before, bar_width, label='Sebelum Tuning', color=['cornflowerblue', 'cornflowerblue'])
plt.bar(index + bar_width, accuracies_after, bar_width, label='Setelah Tuning', color=['darkorange', 'darkorange'])

plt.title('Perbandingan Akurasi Model Sebelum dan Sesudah Hyperparameter Tuning')
plt.xlabel('Model')
plt.ylabel('Akurasi')
plt.xticks(index + bar_width / 2, model_names)
plt.ylim([0, 1])

# Menampilkan nilai akurasi di atas bar dan di bawah diagram
for i, v in enumerate(accuracies_before):
    plt.text(i, v - 0.05, str(round(v, 2)), ha='center', color='black') # Menampilkan di bawah
for i, v in enumerate(accuracies_after):
    plt.text(i + bar_width, v - 0.05, str(round(v, 2)), ha='center', color='black') # Menampilkan di bawah

# Menampilkan keterangan di bawah
plt.legend(loc='upper center', bbox_to_anchor=(0.5, -0.1), ncol=2) # Pindahkan keterangan ke bawah

plt.show()

"""**Prediksi Data test menggunakan Model SVM**"""

#Menghapus fitur yang memiliki korelasi kecil yaitu fitur "n_cores, m_dep, dan clock_speed"
df_test = df_test.drop(['id', 'n_cores', 'm_dep', 'clock_speed'], axis=1)
df_test.head()

# Buat model SVM
model_prediksi = SVC(kernel='poly', C=0.1, gamma=1)

# Latih model pada data latih
model_prediksi.fit(X_train, y_train)

# Lakukan prediksi pada data uji
y_prediksi = model_prediksi.predict(df_test)

# Buat DataFrame dari hasil prediksi
predictions_df = pd.DataFrame(y_prediksi, columns=['price_range'])

# Simpan prediksi ke file CSV
predictions_df.to_csv('/content/drive/MyDrive/Machine Learning/Latihan Dicoding ML/svm_predictions.csv', index=False)
print("Prediksi telah selesai dan disimpan dalam 'svm_predictions.csv'")

#Memanggil data yang sudah diprediksi
prediksi = pd.read_csv('/content/drive/MyDrive/Machine Learning/Latihan Dicoding ML/svm_predictions.csv')
prediksi.head()

# Gabungkan kolom 'price_range' dari hasil prediksi ke DataFrame df_test
df_test['price_range'] = y_prediksi

# Atau, jika ingin melihat dalam bentuk tabel interaktif:
df_test.head()